# RDS生成ツール (Random Dot Stereogram Generator)

関西学院大学 小川洋和研究室で開発された、心理学・認知科学研究用のランダムドットステレオグラム（RDS）生成ツールです。

## 🔬 概要

本ツールは、立体視研究において高精度な視差制御が可能なRDS刺激を生成します。2次元FFT（高速フーリエ変換）に基づく位相シフト法と境界アーチファクト対策を組み合わせることで、サブピクセル精度での正確な視差制御を実現し、従来の手法では困難だった微細な奥行き知覚の研究を可能にします。

## 🧮 技術仕様

### 2D-FFT位相シフト法による視差生成

#### 基本原理
フーリエ変換のシフト定理を2次元に拡張し、画像の位相成分を操作することで、ピクセル以下の精度で空間的なシフトを実現します。

$f(x - d_x, y - d_y) \iff F(u, v) \times e^{-i2\pi(ud_x + vdy)}$

ここで：
- `f(x, y)`: 元の2次元画像
- `F(u, v)`: 2次元フーリエ変換後のスペクトル
- `$d_x, d_y$`: x, y方向のシフト量
- `u, v`: 空間周波数

#### 実装詳細
本ツールでは、理論的に正確でアーチファクトの少ないシフトを実現するために、以下の堅牢な手順を採用しています。

1.  **パディング**: FFTの周期性に起因する境界アーチファクト（ラップアラウンド誤差）を防ぐため、処理前に画像の周囲にパディングを追加します。 (`np.pad`)
2.  **2次元FFT**: 画像全体に対して`fft.fft2`を適用し、一括で周波数領域に変換します。
3.  **中心化**: `fft.fftshift`を使用し、DC成分（ゼロ周波数）をスペクトルの中心に移動させ、物理的に解釈しやすい形で処理を行います。
4.  **位相シフト**: 2次元の周波数座標 `(u,v)` に基づき、目的のシフト量に応じた位相項を計算し、乗算します。
5.  **逆中心化**: `fft.ifftshift`でスペクトルの配置を元に戻します。
6.  **逆2次元FFT**: `fft.ifft2`で画像を空間領域に再構成します。
7.  **クロッピング**: 最初に追加したパディング領域を削除し、画像を元のサイズに戻します。

```python
# 堅牢な2D-FFT位相シフトのコアロジック
padded_image = np.pad(image, pad_width, mode='reflect')
fft_shifted = fft.fftshift(fft.fft2(padded_image))
# ... 周波数座標U, Vの生成 ...
phase_shift_2d = np.exp(-2j * np.pi * U * shift_x)
shifted_fft = fft_shifted * phase_shift_2d
shifted_padded = fft.ifft2(fft.ifftshift(shifted_fft))
shifted_image = shifted_padded_real[pad_width:-pad_width, pad_width:-pad_width]
````

### 視差の単位変換

視覚刺激における視差は、視角（秒角, arcsec）で定義され、以下の式でピクセル値に変換されます：

`視差(pixel) = 視差(arcsec) × 観察距離(cm) × PPI / (180/π × 3600 × 2.54)`

**変換プロセス**:

1.  秒角 → ラジアン: `θ(rad) = θ(arcsec) / 206265`
2.  視角 → 物理距離: `d(cm) = tan(θ) × 観察距離 ≈ θ × 観察距離`
3.  物理距離 → ピクセル: `d(pixel) = d(cm) × (PPI/2.54)`

### ステレオペア生成

左右眼用画像は、基準画像に対して逆方向のシフトを適用して生成：

```
左眼画像 = FFT_shift(基準画像, -視差/2, マスク)
右眼画像 = FFT_shift(基準画像, +視差/2, マスク)
```

この方法により、指定された図形領域のみが立体視で知覚され、背景は平面上に知覚されます。

## 📋 パラメータ詳細

### 画像生成パラメータ

| パラメータ | 範囲 | デフォルト | 説明 |
| :--- | :--- | :--- | :--- |
| **画像サイズ** | 128-1024px | 512×512 | 出力画像の解像度 |
| **ドット密度** | 1-100% | 50% | 画像全体に対するドットの占有率 |
| **ドットサイズ** | 1-10px | 2px | 個々のドットのサイズ |

### 立体視パラメータ

| パラメータ | 範囲 | デフォルト | 説明 |
| :--- | :--- | :--- | :--- |
| **視差** | -600 〜 +600 arcsec | 20 arcsec | 両眼視差（+：飛び出し、-：奥行き） |
| **観察距離** | 30-200 cm | 57 cm | 被験者とモニター間の距離 |
| **モニターPPI** | 72-400 | 96 | モニターの画素密度 |

### 図形パラメータ

| パラメータ | 説明 |
| :--- | :--- |
| **図形形状** | 四角形・円形から選択 |
| **表示モード** | 面（塗りつぶし）・枠線から選択 |
| **枠線太さ** | 枠線モード時の線幅（1-20px） |
| **位置・サイズ** | 中心座標と幅・高さを指定 |

## 🎯 実験での使用方法

### 1\. 基本的な立体視実験

```python
# 標準的な設定例
視差 = 20 arcsec        # 検出閾値付近
観察距離 = 57 cm        # 標準視距離
モニターPPI = 96        # 一般的なモニター
図形 = 四角形, 面       # シンプルな形状
サイズ = 256×256 px     # 視角約4.5度（57cm距離）
```

### 2\. 視差検出閾値の測定

バッチ生成機能を使用して、段階的な視差刺激セットを作成：

```
開始視差: -60 arcsec
終了視差: +60 arcsec  
ステップ: 5 arcsec
→ 25枚の刺激画像を自動生成
```

### 3\. 発達研究での応用

年齢に応じた視差設定の例：

| 対象 | 推奨視差範囲 | 観察距離 |
| :--- | :--- | :--- |
| 成人 | 10-120 arcsec | 57 cm |
| 学童期 | 20-200 arcsec | 50 cm |
| 幼児期 | 40-400 arcsec | 40 cm |

## 🔄 アルゴリズムの利点

### 従来手法との比較

| 項目 | 従来の画素シフト法 | 2D-FFT位相シフト法（本ツール） |
| :--- | :--- | :--- |
| **精度** | 整数ピクセル単位 | **サブピクセル精度** |
| **アーティファクト** | エイリアシング発生 | **パディング処理により最小限** |
| **小視差対応** | 困難 | **高精度で対応** |
| **計算コスト** | 低 | 中程度 |

### 科学的妥当性

  - **再現性**: 同一パラメータから常に同じ刺激を生成。
  - **定量性**: 物理的に意味のある単位（視角）での視差指定。
  - **制御性**: 独立したパラメータによる刺激制御。
  - **正確性**: 2次元FFTによる正確なスペクトル表現と、境界アーチファクトの抑制。

## 📊 出力形式

### ファイル命名規則

```
rds_{符号}{視差値:03d}_{眼}.png
```

**例**:

  - `rds_pos020_L.png`: +20 arcsec, 左眼用
  - `rds_neg060_R.png`: -60 arcsec, 右眼用

### メタデータ

各画像には以下の情報が記録されます：

  - 生成日時
  - 全パラメータ値
  - 算出された物理的視差値（mm, ピクセル）

## 🌐 オンライン版

**URL**: https://rds-generator-ogwlab.streamlit.app

### 特徴

  - インストール不要
  - リアルタイムプレビュー
  - バッチ生成対応
  - 研究室内での共有が容易

## ⚠️ 使用上の注意

### 視覚的制約

  - **観察距離の維持**: 指定した距離での観察が必要。
  - **モニター校正**: 正確なPPI値の設定が重要。
  - **照明条件**: 均一な照明下での実施を推奨。

### 技術的制約

  - **最小視差**: 約0.5ピクセル（モニター依存）。
  - **最大視差**: 融像限界内（一般的に600 arcsec以下）。
  - **画像サイズ**: FFT処理のため2の累乗サイズが効率的。

## 📚 理論的背景

### 立体視の神経メカニズム

RDS刺激は、大脳皮質V1野の両眼視差検出細胞の応答特性を調べるために開発されました（Julesz, 1971）。単眼手がかりを排除することで、純粋な両眼視差による奥行き知覚を研究できます。

### FFT位相シフト法の数学的基礎

フーリエ変換の性質により、空間領域でのシフトは周波数領域での位相回転として表現されます。本ツールで採用する2次元FFTとパディング処理は、この性質を利用し、補間による画質劣化や境界部のアーチファクトを避けながら、高精度なサブピクセルシフトを実現する標準的な手法です（Guizar-Sicairos et al., 2008）。

## 🔗 参考文献

1.  **Guizar-Sicairos, M., Thurman, S. T., & Fienup, J. R. (2008).** Efficient subpixel image registration algorithms. *Optics Letters, 33*(2), 156-158.
2.  **Julesz, B. (1971).** *Foundations of Cyclopean Perception*. University of Chicago Press.
3.  **Poggio, G. F., & Fischer, B. (1977).** Binocular interaction and depth sensitivity in striate and prestriate cortex of behaving rhesus monkey. *Journal of Neurophysiology, 40*(6), 1392-1405.
4.  **Tyler, C. W. (1973).** Stereoscopic vision: cortical limitations and a disparity scaling effect. *Science, 181*(4104), 276-278.

## 💻 技術サポート

### 開発環境

  - **言語**: Python 3.8+
  - **主要ライブラリ**: NumPy, SciPy, Streamlit, Pillow
  - **FFT実装**: SciPy.fft（FFTW ベース）

### バグレポート・機能要望

GitHub Issues: https://github.com/hrkzogw/ogwlab/issues

### 更新履歴

  - **v1.0.0** (2025/06): 初回リリース、2D-FFT位相シフト法および境界アーチファクト対策を実装
  - **v1.1.0** (予定): 時間的変動刺激、複数図形対応

## 👥 開発者・貢献者

**開発**: 小川洋和研究室 - 関西学院大学  
**連絡先**: hirokazu.ogawa@kwansei.ac.jp  
**研究室Webサイト**: https://ogwlab.org/

-----

## 📄 ライセンス

本ソフトウェアは研究・教育目的での使用を前提としています。商用利用については事前にご相談ください。

-----

*本ツールを使用した研究成果を発表される際は、以下の形式での引用をお願いいたします：*

```
小川洋和研究室 (2025). RDS生成ツール (v1.0.0). 関西学院大学. 
Retrieved from [https://github.com/hrkzogw/ogwlab](https://github.com/hrkzogw/ogwlab)
```

```
```
